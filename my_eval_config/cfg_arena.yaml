# input raw data
question_file: /root/lm_eval/evalscope/my_eval_config/question.jsonl

# candidate models to be battled
answers_gen:
    Qwen2-3B-bistu-sft:
        model_id_or_path: /root/llm_model/qwen2.5_3B_bistu_sft
        precision: torch.float16
        enable: true           # enable or disable this model      # TODO: tokenizer issue
        template_type: default-generation
        generation_config:
            do_sample: true
            max_new_tokens: 256
            top_k: 20
            top_p: 0.75
            temperature: 0.3
        output_file: registry/data/arena/answers/answer_Qwen2-3B-bistu-sft.jsonl
    Qwen2-0.5B:
        model_id_or_path: /root/llm_model/qwen2.5_0.5B_model
        # revision: v1.1.8       # revision of model, default is NULL
        precision: torch.float16
        enable: true           # enable or disable this model      # TODO: tokenizer issue
        template_type: default-generation
        generation_config:
            do_sample: true
            max_new_tokens: 256
            top_k: 20
            top_p: 0.75
            temperature: 0.3
        output_file: registry/data/arena/answers/answer_Qwen2-0.5B.jsonl

# Auto-reviewer(GPT-4) config
reviews_gen:
    enable: true
    reviewer:
        # class reference of auto reviewer(GPT-4)
        ref: evalscope.evaluator.reviewer.auto_reviewer:AutoReviewerGpt4
        args:
            max_tokens: 1024
            temperature: 0.2
            # options: pairwise, pairwise_baseline, single (default is pairwise)
            mode: pairwise
            # position bias mitigation strategy, options: swap_position, randomize_order, NULL. default is NULL
            position_bias_mitigation: NULL
            # completion parser config, default is lmsys_parser
            fn_completion_parser: lmsys_parser
    # prompt templates for auto reviewer(GPT-4)
    prompt_file: registry/data/prompt_template/prompt_templates.jsonl
    # target answer files list to be reviewed,
    # could be replaced by your own path: ['/path/to/answers_model_1.jsonl', '/path/to/answers_model_2.jsonl', ...]
    # Default is NULL, which means all answers in answers_gen will be reviewed
    target_answers: NULL
    # output file name of auto reviewer
    review_file: registry/data/arena/reviews/review_gpt4_2.jsonl

# rating results
rating_gen:
    enable: false
    metrics: ['elo']
    # elo rating report file name
    report_file: registry/data/arena/reports/elo_rating_origin.csv
